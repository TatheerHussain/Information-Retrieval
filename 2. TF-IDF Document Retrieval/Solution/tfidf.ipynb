{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gensim.models as models\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "import random,numpy as np\n",
    "import re, string, unicodedata\n",
    "import nltk\n",
    "import contractions\n",
    "import inflect\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "# from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import operator, random\n",
    "import sys\n",
    "import math\n",
    "import pickle\n",
    "from nltk.stem import *\n",
    "from num2words import num2words\n",
    "\n",
    "def remove_header_footer(final_string):\n",
    "\tnew_final_string=\"\"\n",
    "\tflag=1\n",
    "\ttokens=final_string.split('\\n\\n')\n",
    "\t# Remove tokens[0] and tokens[-1]\n",
    "\tfor token in tokens[1:-1]:\n",
    "\t\tflag+=1\n",
    "\t\tnew_final_string+=str(token)+\" \"\n",
    "\tflag=0\n",
    "\treturn new_final_string\n",
    "\n",
    "def remove_html(data):\n",
    "\treturn BeautifulSoup(data, \"html.parser\").get_text()\n",
    "\n",
    "# def remove_btw_sqr(data):\n",
    "#     fin = re.sub('\\[[^]]*\\]', '', data)\n",
    "#     return fin\n",
    "\n",
    "def fix_contractions(data):\n",
    "    fin = contractions.fix(data)\n",
    "    return fin\n",
    "\n",
    "def words_tokenizer(data):\n",
    "\twords = nltk.word_tokenize(data)\n",
    "\t# tknzr = TweetTokenizer()\t\n",
    "\t# tknzr.tokenize(data)\n",
    "\treturn words\n",
    "\n",
    "def remove_non_ascii(words):\n",
    "\tnew_words = []\n",
    "\tflag = 0\n",
    "\tfor i in range(len(words)):\n",
    "\t\tflag = 1\n",
    "\t\tnew_word = unicodedata.normalize('NFKD',unicode(words[i]))\n",
    "\t\tnew_word = new_word.encode('ascii','ignore')\n",
    "\t\tnew_word = new_word.decode('utf-8','ignore')\n",
    "\t\tflag+=1\n",
    "\t\tnew_words.append(new_word)\n",
    "\treturn new_words\n",
    "# def remove_non_ascii(words):\n",
    "#     new_words = []\n",
    "#     flag = 0\n",
    "#     for i in range(len(words)):\n",
    "#  \t\tflag=1\n",
    "#  \t\tnew_word = unicodedata.normalize('NFKD', words[i]).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "#  \t\tflag+=1\n",
    "#  \t\tnew_words.append(new_word)\n",
    "#  \treturn new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    new_words = []\n",
    "    flag = 0\n",
    "    for i in range(len(words)):\n",
    "        new_word = words[i].lower()\n",
    "        flag+=1\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    new_words = []\n",
    "    flag = 0\n",
    "    for i in range(len(words)):\n",
    "    \tflag+=1\n",
    "        new_word = re.sub(r'([^\\w\\s])|_+', '', words[i])\n",
    "        if new_word != '':\n",
    "        \tflag=0\n",
    "        \tnew_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "# def replace_numbers(words):\n",
    "#     p = inflect.engine()\n",
    "#     new_words = []\n",
    "#     flag = 0\n",
    "#     for i in range(len(words)):\n",
    "#     \tflag = 1\n",
    "#         if words[i].isdigit():\n",
    "#             new_word = p.number_to_words(words[i])\n",
    "#             flag+=1\n",
    "#             new_words.append(new_word)\n",
    "#         else:\n",
    "#         \tflag = 0\n",
    "#         \tnew_words.append(words[i])\n",
    "#     return new_words\n",
    "\n",
    "def replace_numbers(words):\n",
    "\tnew_words = []\n",
    "\tfor i in range(len(words)):\n",
    "\t\tif words[i].isdigit():\n",
    "\t\t\ttemp_word = num2words(words[i])\n",
    "\t\t\tnew_words.append(temp_word)\n",
    "\t\telse:\n",
    "\t\t\tnew_words.append(words[i])\n",
    "\treturn new_words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    new_words = []\n",
    "    flag = 0\n",
    "    for i in range(len(words)):\n",
    "    \tflag = 1\n",
    "        if words[i] not in stopwords.words('english'):\n",
    "        \tflag+=1\n",
    "        \tnew_words.append(words[i])\n",
    "    return new_words\n",
    "\n",
    "def stemming(words):\n",
    "\tnew_words = []\n",
    "\tstemmer = PorterStemmer()\n",
    "\tfor i in range(len(words)):\n",
    "\t\tnew_words.append(stemmer.stem(words[i]))\n",
    "\treturn new_words\n",
    "\n",
    "def preprocess_input_sentence(data):\n",
    "\t# data = remove_header_footer(data)\n",
    "\tdata = remove_html(data)\n",
    "\t# data = remove_btw_sqr(data)\n",
    "\tdata = fix_contractions(data)\n",
    "\twords = words_tokenizer(data)\n",
    "\twords = remove_non_ascii(words)\n",
    "\twords = to_lowercase(words)\n",
    "\twords = remove_punctuation(words)\n",
    "\twords = replace_numbers(words)\n",
    "\twords = stemming(words)\n",
    "\twords = remove_stopwords(words)\n",
    "\treturn words\n",
    "\n",
    "file_mapping_count = 0\n",
    "prepro_data_dic = {}\n",
    "count_to_name = {}\n",
    "name_to_count = {}\n",
    "file_titles = {}\n",
    "toremove = []\n",
    "\n",
    "with open('prepro_files.pkl') as f:\n",
    "\tprepro_data_dic = pickle.load(f)\n",
    "\n",
    "with open('name_to_count.pkl') as f:\n",
    "\tname_to_count = pickle.load(f)\n",
    "\n",
    "with open('count_to_name.pkl') as f:\n",
    "\tcount_to_name = pickle.load(f)\n",
    "\n",
    "with open('toremove.pkl') as f:\n",
    "\ttoremove = pickle.load(f)\n",
    "\n",
    "for i in range(len(toremove)):\n",
    "\tdel prepro_data_dic[toremove[i]]\n",
    "\n",
    "with open('count_to_titles.pkl') as f:\n",
    "\tfile_titles = pickle.load(f)\n",
    "\n",
    "\n",
    "# ----------------------file titles -----------------------------\n",
    "# file_titles = []\n",
    "\n",
    "# with open('file_titles.txt') as f:\n",
    "# \tfor line in f:\n",
    "# \t\tfile_titles.append(line)\n",
    "\n",
    "# for i in range(len(file_titles)):\n",
    "# \tif i%2==0:\n",
    "#  \t\tname = file_titles[i].split('\\t')[0]\n",
    "#  \t\tsize = file_titles[i].split('\\t')[1]\n",
    "#  \t\tif name not in titles:\n",
    "#      \t\ttitles[name] = {}\n",
    "#  \t\ttitles[name]['size'] = int(size.rstrip('\\n'))\n",
    "#  \t\t# print titles\n",
    "# \telse:\n",
    "#  \t\ttitles[name]['title'] = file_titles[i]\n",
    "\n",
    "# -----------------------------reading and processing files ----------------------\n",
    "# for i in os.listdir('stories/'):\n",
    "# \ttry:\n",
    "# \t\tfile_name_path = 'stories/'+i\n",
    "# \t\ttemp_data = open(file_name_path,'rb').read().decode('utf-8', 'ignore').lower()\n",
    "# \t\tfile_mapping_count+=1\n",
    "# \t\tfile_name = i\n",
    "# \t\tcount_to_name[file_mapping_count] = file_name\n",
    "# \t\tname_to_count[file_name] = file_mapping_count\n",
    "# \t\tif (\".html\" in file_name or \".descs\" in file_name or \".header\" in file_name or \".footer\" in file_name or \".musings\" in file_name):\n",
    "# \t\t\ttoremove.append(file_mapping_count)\n",
    "# \t\tprepro_data = preprocess_input_sentence(temp_data)\n",
    "# \t\tprepro_data_dic[file_mapping_count] = prepro_data\n",
    "# \texcept:\n",
    "# \t\tpass\n",
    "\n",
    "# for i in os.listdir('stories/'):\n",
    "# \ttry:\n",
    "# \t\tfor j in sorted(os.listdir('stories/'+i)):\n",
    "# \t\t\tfile_name_path = 'stories/'+i+'/'+j\n",
    "# \t\t\ttemp_data = open(file_name_path,'rb').read().decode('utf-8', 'ignore').lower()\n",
    "# \t\t\tfile_mapping_count+=1\n",
    "# \t\t\tfile_name = i+'/'+j\n",
    "# \t\t\tcount_to_name[file_mapping_count] = file_name\n",
    "# \t\t\tname_to_count[file_name] = file_mapping_count\n",
    "# \t\t\tif (\".html\" in file_name or \".descs\" in file_name or \".header\" in file_name or \".footer\" in file_name or \".musings\" in file_name):\n",
    "# \t\t\t\ttoremove.append(file_mapping_count)\n",
    "# \t\t\tprepro_data = preprocess_input_sentence(temp_data)\n",
    "# \t\t\tprepro_data_dic[file_mapping_count] = prepro_data\n",
    "# \texcept:\n",
    "# \t\tpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------- creating dictionaries -----------------------------------\n",
    "\n",
    "vocab = []\n",
    "\n",
    "# for k,v in prepro_data_dic.iteritems():\n",
    "#     for i in range(len(v)):\n",
    "#         if v[i] not in vocab:\n",
    "#             vocab.append(v[i])\n",
    "\n",
    "# with open('vocab.pkl','wb') as f:\n",
    "#     f.write(pickle.dumps(vocab))\n",
    "\n",
    "with open('vocab.pkl') as f:\n",
    "    vocab = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38896"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF Dictionary\n",
    "\n",
    "tf_dic = {}\n",
    "\n",
    "# for k,v in prepro_data_dic.iteritems():\n",
    "#     tf_dic[k] = {}\n",
    "#     for i in range(len(v)):\n",
    "#         if v[i] not in tf_dic[k]:\n",
    "#             tf_dic[k][v[i]] = 0\n",
    "#         tf_dic[k][v[i]] += 1\n",
    "\n",
    "# with open('tf_dic.pkl','wb') as f:\n",
    "#     f.write(pickle.dumps(tf_dic))\n",
    "\n",
    "with open('tf_dic.pkl') as f:\n",
    "    tf_dic = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "467"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tf_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF Dictionary\n",
    "\n",
    "df_dic = {}\n",
    "\n",
    "# for i in range(len(vocab)):\n",
    "#         for k,v in prepro_data_dic.iteritems():\n",
    "#             if vocab[i] in v:\n",
    "#                 if vocab[i] not in df_dic:\n",
    "#                     df_dic[vocab[i]] = []\n",
    "#                 df_dic[vocab[i]].append(k)\n",
    "\n",
    "# with open('df_dic.pkl','wb') as f:\n",
    "#     f.write(pickle.dumps(df_dic))\n",
    "\n",
    "with open('df_dic.pkl') as f:\n",
    "    df_dic = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38896"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [u'fabl', u'ben', u'blumenberg']\n"
     ]
    }
   ],
   "source": [
    "prepro_file_titles = {}\n",
    "\n",
    "for k,v in file_titles.iteritems():\n",
    "    temp = preprocess_input_sentence(v['title'])\n",
    "    prepro_file_titles[k] = temp\n",
    "\n",
    "for k,v in prepro_file_titles.iteritems():\n",
    "    print k,v\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Title document \n",
    "\n",
    "tf_tit_dic = {}\n",
    "# for k,v in prepro_file_titles.iteritems():\n",
    "#     tf_tit_dic[k] = {}\n",
    "#     for i in range(len(v)):\n",
    "#         if v[i] not in tf_tit_dic[k]:\n",
    "#             tf_tit_dic[k][v[i]] = 0\n",
    "#         tf_tit_dic[k][v[i]] += 1\n",
    "\n",
    "# with open('tf_tit_dic.pkl','wb') as f:\n",
    "#     f.write(pickle.dumps(tf_tit_dic))\n",
    "\n",
    "\n",
    "with open('tf_tit_dic.pkl') as f:\n",
    "    tf_tit_dic = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tit_dic = {}\n",
    "\n",
    "# for i in range(len(vocab)):\n",
    "#         for k,v in prepro_file_titles.iteritems():\n",
    "#             if vocab[i] in v:\n",
    "#                 if vocab[i] not in df_tit_dic:\n",
    "#                     df_tit_dic[vocab[i]] = []\n",
    "#                 df_tit_dic[vocab[i]].append(k)\n",
    "\n",
    "# with open('df_tit_dic.pkl','wb') as f:\n",
    "#     f.write(pickle.dumps(df_tit_dic))\n",
    "\n",
    "with open('df_tit_dic.pkl') as f:\n",
    "    df_tit_dic = pickle.load(f) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter query: All summer long, they roamed through the woods and over the plains,playing games and having fun. None were happier than the three little pigs, and they easily made friends with everyone. Wherever they went, they were given a warm  welcome, but as summer drew to a close, they realized that folk were drifting back to their usual jobs, and preparing for winter. Autumn came and it began to rain. The three little pigs started to feel they needed a real home. Sadly they knew that the fun was over now and they must set to work like the others, or they'd be left in the cold and rain, with no roof over their heads. They talked about what to do, but each decided for himself. The laziest little pig said he'd build a straw hut.\n",
      "[u'summer', u'long', u'roam', u'wood', u'plain', u'play', u'game', u'fun', u'none', u'happier', u'three', u'littl', u'pig', u'easili', u'made', u'friend', u'everyon', u'wherev', u'went', u'given', u'warm', u'welcom', u'summer', u'drew', u'close', u'realiz', u'folk', u'drift', u'back', u'usual', u'job', u'prepar', u'winter', u'autumn', u'came', u'began', u'rain', u'three', u'littl', u'pig', u'start', u'feel', u'need', u'real', u'home', u'sadli', u'knew', u'fun', u'wa', u'must', u'set', u'work', u'like', u'would', u'left', u'cold', u'rain', u'roof', u'head', u'talk', u'decid', u'laziest', u'littl', u'pig', u'said', u'would', u'build', u'straw', u'hut']\n"
     ]
    }
   ],
   "source": [
    "# query\n",
    "\n",
    "import math\n",
    "\n",
    "query = str(raw_input('Enter query: '))\n",
    "query = preprocess_input_sentence(query)\n",
    "\n",
    "print query\n",
    "results = {}\n",
    "for i in range(len(query)):\n",
    "    term = query[i]\n",
    "    for k,v in prepro_data_dic.iteritems():\n",
    "        tf_value = 0\n",
    "        if term in tf_dic[k]:\n",
    "            tf_value = tf_dic[k][term]/(len(prepro_data_dic[k])*1.0)\n",
    "        df_value = 0\n",
    "        if term in df_dic:\n",
    "            df_value = len(df_dic[term])\n",
    "        body_score = (1+tf_value)*(math.log10(len(prepro_data_dic)/((1+df_value)*1.0)))\n",
    "        tf_tit_value = 0\n",
    "        if term in tf_tit_dic[k]:\n",
    "            tf_tit_value = tf_tit_dic[k][term]/(len(prepro_file_titles[k])*1.0)\n",
    "        df_tit_value = 0\n",
    "        if term in df_tit_dic:\n",
    "            df_tit_value = len(df_tit_dic[term])\n",
    "        title_score = (1+tf_tit_value)*(math.log10(len(prepro_file_titles)/((1+df_tit_value)*1.0)))\n",
    "        total_score = (0.6*(title_score))+(0.4*(body_score))\n",
    "        if term not in results:\n",
    "            results[term] = {}\n",
    "        results[term][k] = total_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter k for top k documents: 5\n",
      "\n",
      "friends.txt\t104.66715272\n",
      "3lpigs.txt\t104.663319964\n",
      "game.txt\t104.632069561\n",
      "lmtchgrl.txt\t104.490669706\n",
      "3wishes.txt\t104.36349551\n"
     ]
    }
   ],
   "source": [
    "k_value = int(raw_input(\"Enter k for top k documents: \"))\n",
    "\n",
    "combine_results = {}\n",
    "for k,v in results.iteritems():\n",
    "    for key, value in v.iteritems():\n",
    "        if key not in combine_results:\n",
    "            combine_results[key] = 0\n",
    "        combine_results[key]+=value\n",
    "\n",
    "sorted_results = sorted(combine_results.items(), key=operator.itemgetter(1),reverse=True)\n",
    "# print sorted_results\n",
    "sorted_results = sorted_results[:k_value]\n",
    "\n",
    "print \"\"\n",
    "for i in range(len(sorted_results)):\n",
    "    print str(count_to_name[sorted_results[i][0]])+\"\\t\"+str(sorted_results[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Part 2: Tf-Idf based vector space document retrieval:\n",
    "\n",
    "tfresults = {}\n",
    "# query = vocab\n",
    "# for i in range(len(query)):\n",
    "#     term = query[i]\n",
    "#     for k,v in prepro_data_dic.iteritems():\n",
    "#         tf_value = 0\n",
    "#         if term in tf_dic[k]:\n",
    "#             tf_value = tf_dic[k][term]/(len(prepro_data_dic[k])*1.0)\n",
    "#         df_value = 0\n",
    "#         if term in df_dic:\n",
    "#             df_value = len(df_dic[term])\n",
    "#         body_score = (1+tf_value)*(math.log10(len(prepro_data_dic)/((1+df_value)*1.0)))\n",
    "#         tf_tit_value = 0\n",
    "#         if term in tf_tit_dic[k]:\n",
    "#             tf_tit_value = tf_tit_dic[k][term]/(len(prepro_file_titles[k])*1.0)\n",
    "#         df_tit_value = 0\n",
    "#         if term in df_tit_dic:\n",
    "#             df_tit_value = len(df_tit_dic[term])\n",
    "#         title_score = (1+tf_tit_value)*(math.log10(len(prepro_file_titles)/((1+df_tit_value)*1.0)))\n",
    "#         total_score = (0.6*(title_score))+(0.4*(body_score))\n",
    "#         if k not in tfresults:\n",
    "#             tfresults[k] = []\n",
    "#         tfresults[k].append(total_score)\n",
    "# #         tfresults[k].append((term,total_score))\n",
    "\n",
    "# with open('document_term_tfidf_without_title.pkl','wb') as f:\n",
    "#     f.write(pickle.dumps(tfresults))\n",
    "\n",
    "with open('document_term_tfidf.pkl') as f:\n",
    "    tfresults = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter query: All summer long, they roamed through the woods and over the plains,playing games and having fun. None were happier than the three little pigs, and they easily made friends with everyone. Wherever they went, they were given a warm  welcome, but as summer drew to a close, they realized that folk were drifting back to their usual jobs, and preparing for winter. Autumn came and it began to rain. The three little pigs started to feel they needed a real home. Sadly they knew that the fun was over now and they must set to work like the others, or they'd be left in the cold and rain, with no roof over their heads. They talked about what to do, but each decided for himself. The laziest little pig said he'd build a straw hut.\n"
     ]
    }
   ],
   "source": [
    "query = str(raw_input('Enter query: '))\n",
    "query = preprocess_input_sentence(query)\n",
    "\n",
    "query_frequency = {}\n",
    "\n",
    "for i in range(len(query)):\n",
    "    if query[i] not in query_frequency:\n",
    "        query_frequency[query[i]]=0\n",
    "    query_frequency[query[i]]+=1\n",
    "\n",
    "query_vector = []\n",
    "for i in range(len(vocab)):\n",
    "    if vocab[i] in query_frequency:\n",
    "        query_vector.append(query_frequency[vocab[i]])\n",
    "    else:\n",
    "        query_vector.append(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38896"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(query_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine similarity\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def cos_sim(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "cosine_sim_results = {}\n",
    "\n",
    "query_vector = np.array(query_vector)\n",
    "\n",
    "for k,v in tfresults.iteritems():\n",
    "    doc_vector = np.array(v)\n",
    "    simval = cos_sim(query_vector,doc_vector)\n",
    "    cosine_sim_results[k] = simval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "467"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cosine_sim_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter k for top k documents: 5\n",
      "\n",
      "3lpigs.txt\t0.026160113489043714\n",
      "lmtchgrl.txt\t0.02606002061115806\n",
      "goldfish.txt\t0.025955042202237533\n",
      "lmermaid.txt\t0.02595375715994727\n",
      "wlgirl.txt\t0.025903164011232195\n"
     ]
    }
   ],
   "source": [
    "k_value = int(raw_input(\"Enter k for top k documents: \"))\n",
    "\n",
    "sorted_sim_results = sorted(cosine_sim_results.items(), key=operator.itemgetter(1),reverse=True)\n",
    "sorted_sim_results = sorted_sim_results[:k_value]\n",
    "\n",
    "print \"\"\n",
    "for i in range(len(sorted_sim_results)):\n",
    "    print str(count_to_name[sorted_sim_results[i][0]])+\"\\t\"+str(sorted_sim_results[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
