{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gensim.models as models\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "import random,numpy as np\n",
    "import re, string, unicodedata\n",
    "import nltk\n",
    "import contractions\n",
    "import inflect\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "# from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import operator, random\n",
    "import sys\n",
    "import math\n",
    "import pickle\n",
    "from nltk.stem import *\n",
    "from num2words import num2words\n",
    "from wordcloud import STOPWORDS\n",
    "\n",
    "def remove_header_footer(final_string):\n",
    "\tnew_final_string=\"\"\n",
    "\tflag=1\n",
    "\ttokens=final_string.split('\\n\\n')\n",
    "\t# Remove tokens[0] and tokens[-1]\n",
    "\tfor token in tokens[1:-1]:\n",
    "\t\tflag+=1\n",
    "\t\tnew_final_string+=str(token)+\" \"\n",
    "\tflag=0\n",
    "\treturn new_final_string\n",
    "\n",
    "def remove_html(data):\n",
    "\treturn BeautifulSoup(data, \"html.parser\").get_text()\n",
    "\n",
    "# def remove_btw_sqr(data):\n",
    "#     fin = re.sub('\\[[^]]*\\]', '', data)\n",
    "#     return fin\n",
    "\n",
    "def fix_contractions(data):\n",
    "    fin = contractions.fix(data)\n",
    "    return fin\n",
    "\n",
    "def words_tokenizer(data):\n",
    "\twords = nltk.word_tokenize(data)\n",
    "\t# tknzr = TweetTokenizer()\t\n",
    "\t# tknzr.tokenize(data)\n",
    "\treturn words\n",
    "\n",
    "def remove_non_ascii(words):\n",
    "\tnew_words = []\n",
    "\tflag = 0\n",
    "\tfor i in range(len(words)):\n",
    "\t\tflag = 1\n",
    "\t\tnew_word = unicodedata.normalize('NFKD',unicode(words[i]))\n",
    "\t\tnew_word = new_word.encode('ascii','ignore')\n",
    "\t\tnew_word = new_word.decode('utf-8','ignore')\n",
    "\t\tflag+=1\n",
    "\t\tnew_words.append(new_word)\n",
    "\treturn new_words\n",
    "# def remove_non_ascii(words):\n",
    "#     new_words = []\n",
    "#     flag = 0\n",
    "#     for i in range(len(words)):\n",
    "#  \t\tflag=1\n",
    "#  \t\tnew_word = unicodedata.normalize('NFKD', words[i]).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "#  \t\tflag+=1\n",
    "#  \t\tnew_words.append(new_word)\n",
    "#  \treturn new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    new_words = []\n",
    "    flag = 0\n",
    "    for i in range(len(words)):\n",
    "        new_word = words[i].lower()\n",
    "        flag+=1\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    new_words = []\n",
    "    flag = 0\n",
    "    for i in range(len(words)):\n",
    "    \tflag+=1\n",
    "        new_word = re.sub(r'([^\\w\\s])|_+', '', words[i])\n",
    "        if new_word != '':\n",
    "        \tflag=0\n",
    "        \tnew_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "# def replace_numbers(words):\n",
    "#     p = inflect.engine()\n",
    "#     new_words = []\n",
    "#     flag = 0\n",
    "#     for i in range(len(words)):\n",
    "#     \tflag = 1\n",
    "#         if words[i].isdigit():\n",
    "#             new_word = p.number_to_words(words[i])\n",
    "#             flag+=1\n",
    "#             new_words.append(new_word)\n",
    "#         else:\n",
    "#         \tflag = 0\n",
    "#         \tnew_words.append(words[i])\n",
    "#     return new_words\n",
    "\n",
    "def replace_numbers(words):\n",
    "\tnew_words = []\n",
    "\tfor i in range(len(words)):\n",
    "\t\tif words[i].isdigit():\n",
    "\t\t\ttemp_word = num2words(words[i])\n",
    "\t\t\tnew_words.append(temp_word)\n",
    "\t\telse:\n",
    "\t\t\tnew_words.append(words[i])\n",
    "\treturn new_words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    new_words = []\n",
    "    flag = 0\n",
    "    for i in range(len(words)):\n",
    "    \tflag = 1\n",
    "        if words[i] not in stopwords.words('english') and words[i] not in STOPWORDS:\n",
    "        \tflag+=1\n",
    "        \tnew_words.append(words[i])\n",
    "    return new_words\n",
    "\n",
    "def remove_numbers(words):\n",
    "    new_words = []\n",
    "    flag=0\n",
    "    for i in range(len(words)):\n",
    "        if words[i].isdigit():\n",
    "            flag+=1\n",
    "        else:\n",
    "            new_words.append(words[i])\n",
    "    return new_words\n",
    "\n",
    "def stemming(words):\n",
    "\tnew_words = []\n",
    "\tstemmer = PorterStemmer()\n",
    "\tfor i in range(len(words)):\n",
    "\t\tnew_words.append(stemmer.stem(words[i]))\n",
    "\treturn new_words\n",
    "\n",
    "def preprocess_input_sentence(data):\n",
    "\t# data = remove_header_footer(data)\n",
    "\tdata = remove_html(data)\n",
    "\t# data = remove_btw_sqr(data)\n",
    "\tdata = fix_contractions(data)\n",
    "\twords = words_tokenizer(data)\n",
    "\twords = remove_non_ascii(words)\n",
    "\twords = to_lowercase(words)\n",
    "\twords = remove_punctuation(words)\n",
    "\twords = replace_numbers(words)\n",
    "\twords = stemming(words)\n",
    "\twords = remove_stopwords(words)\n",
    "\treturn words\n",
    "\n",
    "file_mapping_count = -1\n",
    "prepro_data_dic = {}\n",
    "count_to_name = {}\n",
    "name_to_count = {}\n",
    "file_titles = {}\n",
    "toremove = []\n",
    "ground_labels = []\n",
    "\n",
    "with open('prepro_files.pkl') as f:\n",
    "\tprepro_data_dic = pickle.load(f)\n",
    "\n",
    "# with open('name_to_count.pkl') as f:\n",
    "# \tname_to_count = pickle.load(f)\n",
    "\n",
    "# with open('count_to_name.pkl') as f:\n",
    "# \tcount_to_name = pickle.load(f)\n",
    "\n",
    "with open('ground_labels.pkl') as f:\n",
    "\tground_labels = pickle.load(f)\n",
    "\n",
    "# ground_count = -1\n",
    "# for i in os.listdir('20_newsgroups/'):\n",
    "#     ground_count+=1\n",
    "#     for j in sorted(os.listdir('20_newsgroups/'+i)):\n",
    "#         file_mapping_count+=1\n",
    "# #         print file_mapping_count\n",
    "#         file_name = i+'/'+j\n",
    "# #         print file_name\n",
    "#         count_to_name[file_mapping_count] = file_name\n",
    "#         name_to_count[file_name] = file_mapping_count\n",
    "#         file_name_path = '20_newsgroups/'+i+'/'+j\n",
    "#         temp_data = open(file_name_path,'rb').read().decode('utf-8', 'ignore').lower()\n",
    "#         prepro_data = preprocess_input_sentence(temp_data)\n",
    "#         prepro_data_dic[file_mapping_count] = prepro_data\n",
    "#         ground_labels.append(ground_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Splitting Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def data_splitting(data,labels,ratio,seed):\n",
    "    \n",
    "    classwise_data = {}\n",
    "    for i in range(len(labels)):\n",
    "        if labels[i] not in classwise_data:\n",
    "            classwise_data[labels[i]] = []\n",
    "        classwise_data[labels[i]].append(data[i])\n",
    "    \n",
    "    final_train_data = []\n",
    "    final_test_data = []\n",
    "    final_train_labels = []\n",
    "    final_test_labels = []\n",
    "    final_train_docids = []\n",
    "    final_test_docids = []\n",
    "    \n",
    "    for k,v in classwise_data.iteritems():\n",
    "        shuf_ind = []\n",
    "        for i in range(len(v)):\n",
    "            shuf_ind.append(i)\n",
    "        random.Random(seed).shuffle(shuf_ind)\n",
    "        \n",
    "        lim = int(len(shuf_ind)*ratio)\n",
    "        train_ind = shuf_ind[:lim]\n",
    "        test_ind = shuf_ind[lim:]\n",
    "\n",
    "        train_data = []\n",
    "        train_labels = []\n",
    "        test_data = []\n",
    "        test_labels = []\n",
    "        for i in range(len(train_ind)):\n",
    "            train_data.append(classwise_data[k][train_ind[i]])\n",
    "            train_labels.append(k)\n",
    "        for i in range(len(test_ind)):\n",
    "            test_data.append(classwise_data[k][test_ind[i]])\n",
    "            test_labels.append(k)\n",
    "        final_train_data+=train_data\n",
    "        final_test_data+=test_data\n",
    "        final_train_labels+=train_labels\n",
    "        final_test_labels+=test_labels\n",
    "        \n",
    "    return final_train_data,final_train_labels,final_test_data,final_test_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fucntions to check different data stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data,train_labels,test_data,test_labels = data_splitting(prepro_data_dic,ground_labels,0.7)\n",
    "\n",
    "classwise_vocab = {}\n",
    "\n",
    "for i in range(len(prepro_data_dic)):\n",
    "#         term = prepro_data_dic[i]\n",
    "        label = ground_labels[i]\n",
    "        if label not in classwise_vocab:\n",
    "            classwise_vocab[label] = {}\n",
    "        for j in range(len(prepro_data_dic[i])):\n",
    "            term = prepro_data_dic[i][j]\n",
    "            if term not in classwise_vocab[label]:\n",
    "                classwise_vocab[label][term] = 0\n",
    "            classwise_vocab[label][term]+=1\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 17391\n",
      "1 20192\n",
      "2 20949\n",
      "3 20262\n",
      "4 20170\n"
     ]
    }
   ],
   "source": [
    "for k,v in classwise_vocab.iteritems():\n",
    "    print k, len(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating vocabulary\n",
    "vocab = set()\n",
    "for k,v in classwise_vocab.iteritems():\n",
    "    for key, val in v.iteritems():\n",
    "        vocab.add(key)\n",
    "vocab = list(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68215"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Navie Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio: 0.5\n",
      "Accuracy: 95.2%\n",
      "True vs Predicted\n",
      "[[493.   1.   0.   3.   3.]\n",
      " [  0. 482.   7.   5.   6.]\n",
      " [  1.  10. 477.   7.   5.]\n",
      " [  5.  24.  25. 439.   7.]\n",
      " [  1.   5.   5.   0. 489.]]\n",
      "\n",
      "Ratio: 0.7\n",
      "Accuracy: 95.7333333333%\n",
      "True vs Predicted\n",
      "[[297.   0.   0.   1.   2.]\n",
      " [  0. 284.   4.   9.   3.]\n",
      " [  1.   4. 288.   6.   1.]\n",
      " [  1.  10.  11. 277.   1.]\n",
      " [  1.   2.   7.   0. 290.]]\n",
      "\n",
      "Ratio: 0.8\n",
      "Accuracy: 96.1%\n",
      "True vs Predicted\n",
      "[[200.   0.   0.   0.   0.]\n",
      " [  0. 187.   4.   7.   2.]\n",
      " [  1.   1. 194.   4.   0.]\n",
      " [  0.   6.   6. 187.   1.]\n",
      " [  1.   2.   4.   0. 193.]]\n",
      "\n",
      "Ratio: 0.9\n",
      "Accuracy: 98.0%\n",
      "True vs Predicted\n",
      "[[100.   0.   0.   0.   0.]\n",
      " [  0.  95.   1.   4.   0.]\n",
      " [  0.   0.  99.   1.   0.]\n",
      " [  0.   1.   2.  97.   0.]\n",
      " [  0.   0.   0.   1.  99.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ----------- Testing for different ratio valuex ------------------\n",
    "\n",
    "ratios = [0.5,0.7,0.8,0.9]\n",
    "\n",
    "for rat in range(len(ratios)):\n",
    "    ratio = ratios[rat]\n",
    "    train_data,train_labels,test_data,test_labels = data_splitting(prepro_data_dic,ground_labels,ratio,6)\n",
    "    print \"Ratio: \"+str(ratios[rat])\n",
    "    train_classwise_vocab = {}\n",
    "    train_vocab = set()\n",
    "    for i in range(len(train_data)):\n",
    "        \n",
    "        class_val = train_labels[i]\n",
    "        if class_val not in train_classwise_vocab:\n",
    "            train_classwise_vocab[class_val] = {}\n",
    "        \n",
    "        for j in range(len(train_data[i])):\n",
    "            \n",
    "            term = train_data[i][j]\n",
    "            train_vocab.add(term)\n",
    "            \n",
    "            if term not in train_classwise_vocab[class_val]: \n",
    "                train_classwise_vocab[class_val][term] = 0\n",
    "            train_classwise_vocab[class_val][term]+=1\n",
    "            \n",
    "    train_vocab = list(train_vocab)\n",
    "    \n",
    "#     print \"Train vocab built.\"\n",
    "    \n",
    "    # ---------------- Training -----------------------------\n",
    "    \n",
    "    # prior probabilities\n",
    "\n",
    "    train_prior = {}\n",
    "    for i in range(len(train_labels)):\n",
    "        temp_class = train_labels[i]\n",
    "        if temp_class not in train_prior:\n",
    "            train_prior[temp_class] = 0\n",
    "        train_prior[temp_class]+=1\n",
    "\n",
    "    prior = {}\n",
    "    for k,v in train_prior.iteritems():\n",
    "        prior[k] = np.log((train_prior[k]*1.0)/(len(train_data)))\n",
    "    \n",
    "#     print \"Priors generated.\"\n",
    "    \n",
    "    cond_prob = {}\n",
    "    for i in range(len(train_classwise_vocab)):\n",
    "        \n",
    "        class_val = i\n",
    "        class_terms = train_classwise_vocab[class_val]\n",
    "        \n",
    "        tsum = 0\n",
    "        for k,v in class_terms.iteritems():\n",
    "            tsum+=v\n",
    "        \n",
    "        for k,v in class_terms.iteritems():\n",
    "            if class_val not in cond_prob:\n",
    "                cond_prob[class_val] = {}\n",
    "            cond_prob[class_val][k] = np.log(((v*1.0)+1)/(tsum+len(train_vocab)))\n",
    "    \n",
    "#     print \"Train conditional probabilities generated.\"\n",
    "\n",
    "    # ----------------- Testing --------------------- \n",
    "\n",
    "    test_docs = {}\n",
    "\n",
    "    for i in range(len(test_data)):\n",
    "        if test_labels[i] not in test_docs:\n",
    "            test_docs[test_labels[i]] = []\n",
    "        test_docs[test_labels[i]].append(test_data[i])\n",
    "\n",
    "    classwise_sum = {}\n",
    "    for k,v in classwise_vocab.iteritems():\n",
    "        if k not in classwise_sum:\n",
    "            classwise_sum[k] = 0\n",
    "        for key,val in v.iteritems():\n",
    "            classwise_sum[k]+=val\n",
    "    \n",
    "    cor_count=0\n",
    "    true_class = []\n",
    "    predicted_class = []\n",
    "    for i in range(len(test_data)):\n",
    "        ground_label = test_labels[i]\n",
    "        doc_val = test_data[i]\n",
    "        score = {}\n",
    "        max_val = -float(\"inf\")\n",
    "        max_class = -1\n",
    "        for j in range(5):\n",
    "            score[j] = 0\n",
    "            score[j]+=prior[j]\n",
    "            for term in doc_val:\n",
    "                if term not in cond_prob[j]:\n",
    "                    score[j]+=np.log(1.0/len(train_vocab))*100\n",
    "                else:\n",
    "                    score[j]+=cond_prob[j][term]\n",
    "                    \n",
    "            if score[j]>max_val:\n",
    "                max_val=score[j]\n",
    "                max_class=j\n",
    "        if max_class == ground_label:\n",
    "            cor_count+=1\n",
    "        predicted_class.append(max_class)\n",
    "        true_class.append(ground_label)\n",
    "    \n",
    "    print \"Accuracy: \" + str(((cor_count*1.0)/len(test_data))*100) + \"%\"\n",
    "\n",
    "    # -------------- Confusion Matrix ---------------------\n",
    "\n",
    "    mat = np.zeros((len(train_classwise_vocab), len(train_classwise_vocab)))\n",
    "    flag=0\n",
    "    for it in range(len(predicted_class)):\n",
    "        flag+=1\n",
    "        mat[int(true_class[it])][int(predicted_class[it])] += 1\n",
    "\n",
    "    df = pd.DataFrame(mat)\n",
    "    print(\"True vs Predicted\")\n",
    "    flag+=1\n",
    "    print(mat)\n",
    "    print \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: TF-IDF scoring for feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "ratio = 0.7\n",
    "train_data,train_labels,test_data,test_labels = data_splitting(prepro_data_dic,ground_labels,ratio,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_classwise_vocab = {}\n",
    "train_vocab = set()\n",
    "for i in range(len(train_data)):\n",
    "\n",
    "    class_val = train_labels[i]\n",
    "    if class_val not in train_classwise_vocab:\n",
    "        train_classwise_vocab[class_val] = {}\n",
    "\n",
    "    for j in range(len(train_data[i])):\n",
    "\n",
    "        term = train_data[i][j]\n",
    "        train_vocab.add(term)\n",
    "\n",
    "        if term not in train_classwise_vocab[class_val]: \n",
    "            train_classwise_vocab[class_val][term] = 0\n",
    "        train_classwise_vocab[class_val][term]+=1\n",
    "\n",
    "train_vocab = list(train_vocab)\n",
    "\n",
    "train_classwise_sum = {}\n",
    "\n",
    "for k,v in train_classwise_vocab.iteritems():\n",
    "    if k not in train_classwise_sum:\n",
    "        train_classwise_sum[k] = 0\n",
    "    for key,val in v.iteritems():\n",
    "        train_classwise_sum[k]+=val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- TERM FREQUENCY(TF) --------------------\n",
    "\n",
    "# TF = {}\n",
    "\n",
    "# train_data_freq = {}\n",
    "# for i in range(len(train_data)):\n",
    "#     for j in range(len(train_data[i])):\n",
    "#         term = train_data[i][j]\n",
    "#         if i not in train_data_freq:\n",
    "#             train_data_freq[i] = {}\n",
    "#         if term not in train_data_freq[i]:\n",
    "#             train_data_freq[i][term] = 0\n",
    "#         train_data_freq[i][term]+=1\n",
    "\n",
    "# for i in range(len(train_vocab)):\n",
    "#     term = train_vocab[i]\n",
    "#     for j in range(len(train_data)):\n",
    "#         try:\n",
    "#             tfval = (train_data_freq[j][term]*1.0)/(len(train_data[j]))\n",
    "#             if term not in TF:\n",
    "#                 TF[term] = 0\n",
    "#             TF[term]+=tfval\n",
    "#         except:\n",
    "#             pass\n",
    "\n",
    "# with open('TF_short.pkl','wb') as f:\n",
    "#     f.write(pickle.dumps(TF))\n",
    "\n",
    "# with open('TF_short.pkl') as f:\n",
    "#     TF = pickle.load(f)\n",
    "    \n",
    "# --------------------- DOCUMENT FREQUENCY(DF) -----------------------------\n",
    "\n",
    "# df_dic = {}\n",
    "\n",
    "# for i in range(len(train_vocab)):\n",
    "#     termval = train_vocab[i]\n",
    "#     for j in range(len(train_data)):\n",
    "#         docval = train_data[j]\n",
    "#         if termval in docval:\n",
    "#             if termval not in df_dic:\n",
    "#                 df_dic[termval] = []\n",
    "#             df_dic[termval].append(1)\n",
    "\n",
    "# with open('DF_short.pkl','wb') as f:\n",
    "#     f.write(pickle.dumps(df_dic))\n",
    "\n",
    "# with open('DF_short.pkl') as f:\n",
    "#     DF_classwise = pickle.load(f)\n",
    "\n",
    "# ------------------------ TF-IDF ----------------------------\n",
    "\n",
    "# TFIDF = {}\n",
    "# for i in range(len(train_vocab)):\n",
    "#     term = train_vocab[i]\n",
    "#     tf_value = TF[term]\n",
    "#     df_value = len(df_dic[term])\n",
    "#     body_score = (1+tf_value)*(math.log10(len(train_data)/((1+df_value)*1.0)))\n",
    "#     TFIDF[term] = body_score\n",
    "\n",
    "# with open('TFIDF_short.pkl') as f:\n",
    "#     TFIDF = pickle.load(f)\n",
    "\n",
    "# ------------------- TOP-K FEATURE SELECTION ----------------------------\n",
    "# import operator\n",
    "# sorted_sim_results = sorted(TFIDF.items(), key=operator.itemgetter(1),reverse=True)\n",
    "\n",
    "# top_ind = len(sorted_sim_results)*60/100\n",
    "# topk = sorted_sim_results[:top_ind]\n",
    "# topk_dic = {}\n",
    "\n",
    "# for i in range(len(topk)):\n",
    "#     topk_dic[topk[i][0]] = topk[i][1]\n",
    "\n",
    "# with open('topk_tfidf.pkl','wb') as f:\n",
    "#     f.write(pickle.dumps(topk_dic))\n",
    "\n",
    "topk_dic = {}\n",
    "with open('topk_tfidf.pkl') as f:\n",
    "    topk_dic = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio: 0.7\n",
      "Accuracy: 96.2%\n",
      "True vs Predicted\n",
      "[[295.   0.   0.   1.   4.]\n",
      " [  0. 282.   5.   7.   6.]\n",
      " [  1.   5. 285.   6.   3.]\n",
      " [  0.   2.  10. 287.   1.]\n",
      " [  1.   2.   3.   0. 294.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ---------------- Training -----------------------------\n",
    "print \"Ratio: \" + str(ratio)\n",
    "# prior probabilities\n",
    "\n",
    "train_prior = {}\n",
    "for i in range(len(train_labels)):\n",
    "    temp_class = train_labels[i]\n",
    "    if temp_class not in train_prior:\n",
    "        train_prior[temp_class] = 0\n",
    "    train_prior[temp_class]+=1\n",
    "\n",
    "prior = {}\n",
    "for k,v in train_prior.iteritems():\n",
    "    prior[k] = np.log((train_prior[k]*1.0)/(len(train_data)))\n",
    "\n",
    "# print \"Priors generated.\"\n",
    "\n",
    "cond_prob = {}\n",
    "for i in range(len(train_classwise_vocab)):\n",
    "\n",
    "    class_val = i\n",
    "    class_terms = train_classwise_vocab[class_val]\n",
    "\n",
    "    tsum = 0\n",
    "    for k,v in class_terms.iteritems():\n",
    "        tsum+=v\n",
    "\n",
    "    for k,v in class_terms.iteritems():\n",
    "        if class_val not in cond_prob:\n",
    "            cond_prob[class_val] = {}\n",
    "        cond_prob[class_val][k] = np.log(((v*1.0)+1)/(tsum+len(train_vocab)))\n",
    "\n",
    "# print \"Train conditional probabilities generated.\"\n",
    "\n",
    "# ----------------- Testing --------------------- \n",
    "\n",
    "test_docs = {}\n",
    "\n",
    "for i in range(len(test_data)):\n",
    "    if test_labels[i] not in test_docs:\n",
    "        test_docs[test_labels[i]] = []\n",
    "    test_docs[test_labels[i]].append(test_data[i])\n",
    "\n",
    "classwise_sum = {}\n",
    "for k,v in classwise_vocab.iteritems():\n",
    "    if k not in classwise_sum:\n",
    "        classwise_sum[k] = 0\n",
    "    for key,val in v.iteritems():\n",
    "        classwise_sum[k]+=val\n",
    "\n",
    "cor_count=0\n",
    "true_class = []\n",
    "predicted_class = []\n",
    "for i in range(len(test_data)):\n",
    "    ground_label = test_labels[i]\n",
    "    doc_val = test_data[i]\n",
    "    score = {}\n",
    "    max_val = -float(\"inf\")\n",
    "    max_class = -1\n",
    "    for j in range(5):\n",
    "        score[j] = 0\n",
    "        score[j]+=prior[j]\n",
    "        for term in doc_val:\n",
    "            if (term in topk_dic) and (term in cond_prob[j]):\n",
    "                score[j]+=cond_prob[j][term]\n",
    "            else:\n",
    "                score[j]+=np.log(1.0/len(train_vocab))*100\n",
    "        if score[j]>max_val:\n",
    "            max_val=score[j]\n",
    "            max_class=j\n",
    "    if max_class == ground_label:\n",
    "        cor_count+=1\n",
    "    predicted_class.append(max_class)\n",
    "    true_class.append(ground_label)\n",
    "\n",
    "# print cor_count\n",
    "print \"Accuracy: \" + str(((cor_count*1.0)/len(test_data))*100) + \"%\"\n",
    "\n",
    "# -------------- Confusion Matrix ---------------------\n",
    "\n",
    "mat = np.zeros((len(train_classwise_vocab), len(train_classwise_vocab)))\n",
    "flag=0\n",
    "for it in range(len(predicted_class)):\n",
    "    flag+=1\n",
    "    mat[int(true_class[it])][int(predicted_class[it])] += 1\n",
    "\n",
    "df = pd.DataFrame(mat)\n",
    "print(\"True vs Predicted\")\n",
    "flag+=1\n",
    "print(mat)\n",
    "# print len(train_vocab)\n",
    "print \"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
